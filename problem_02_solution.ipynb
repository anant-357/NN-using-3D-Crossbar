{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"right\">by <a href=\"http://cse.iitkgp.ac.in/~adas/\">Abir Das</a> with help of <br> Ram Rakesh and Ankit Singh<br> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the following details here\n",
    "** Name: ** `DEV NARAYAN YADAV`<br/>\n",
    "** Roll Number: ** `18CS92R03`<br/>\n",
    "** Department: ** `COMPUTER SCIENCE AND ENGINEERING`<br/>\n",
    "** Email: ** `devyadav15294@gmail.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file ([`assignment_01.ipynb`]()) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: You will implement a fully connected neural network from scratch in this problem\n",
    "We marked places where you are expected to add/change your own code with **`##### write your code below #####`** comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "597wDiAvGvuB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 15:12:54.946198: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 15:12:54.948590: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-12 15:12:54.985198: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 15:12:54.985228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 15:12:54.986230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 15:12:54.992765: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-12 15:12:54.993253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 15:12:55.923617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are not supposed to import any other python library to work on this assignments.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "'''You are not supposed to import any other python library to work on this assignments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "8c59bd48-230d-4fb9-eba1-82471de363df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Training Data Shape (X): (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = np.load('./X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "gt_indices = y_train\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))\n",
    "print(\"Training Data Shape (X):\",x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating your model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape (Y): (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "# print(y.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int)\n",
    "print(\"Training Data Shape (Y):\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "4ecb1bfc-4568-44cb-e109-57677da50eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Use the dimesnions specified in the cell 3 to initialize your weights matrices. \n",
    "Use the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "\n",
    "########################## write your code below ##############################################\n",
    "W1 = np.random.normal(0.0,1.0,(input_dim,hidden_1_dim))\n",
    "W2 = np.random.normal(0.0,1.0,(hidden_1_dim,hidden_2_dim))\n",
    "W3 = np.random.normal(0.0,1.0,(hidden_2_dim,output_dim))\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Write a function which computes the softmax where X is vector of scores computed during forward pass\n",
    "def softmax(x):\n",
    "    ##############################write your code here #################################\n",
    "    ab = []\n",
    "    for i in range(0, len(x)):\n",
    "        a = x[i]\n",
    "        shift_x = a-np.max(a)\n",
    "        ab.append(np.exp(shift_x)/np.sum(np.exp(shift_x)))\n",
    "    ab = np.asarray(ab)\n",
    "    return ab\n",
    "    ####################################################################################\n",
    "#x = [1, 2, 0.5, 3, 0.8]\n",
    "#print(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "341578db-29a4-48ca-b0f8-a0343aadd24b"
   },
   "outputs": [],
   "source": [
    "def compute_act(weight, train_data,batch_len, batch_wid):\n",
    "    a = []\n",
    "    for i in range(0, batch_len):\n",
    "        ab = np.matmul(weight.T,train_data[i])\n",
    "        a.append(ab)\n",
    "    a = np.asarray(a)\n",
    "    return a\n",
    "\n",
    "def grad_softmax(pred, onehot,gt_index,b_size):\n",
    "    grad_soft = np.sum(np.diag(np.take((pred - onehot),gt_index, axis=1)))/b_size\n",
    "    return grad_soft\n",
    "\n",
    "def grad_W_3(h2, grad_sm, W3):\n",
    "    grad = np.sum((grad_sm * h2.T).T, axis = 0) * np.ones((W3.shape[1], 1))\n",
    "    grad = grad.T\n",
    "    return grad\n",
    "\n",
    "def grad_hidd2(pred,onehot,W,batch_len):\n",
    "    grad = []\n",
    "    for i in range(0, batch_len):\n",
    "        ab = np.matmul((pred - onehot)[i], W.T)\n",
    "        grad.append(ab)\n",
    "    grad = np.asarray(grad)\n",
    "    return grad\n",
    "\n",
    "def mask(a):\n",
    "    np.place(a, a<0, 0)\n",
    "    return a\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def grad_W(h1,hidden_1,grad_a,hidden_2,batch_len):   \n",
    "    grad = np.sum(np.array([np.matmul(h1[i].reshape((hidden_1, 1)), grad_a[i].reshape((1, hidden_2))) for i in range(batch_size)]), axis=0)/batch_size\n",
    "    return grad\n",
    "\n",
    "def grad_hidd1(grad_a,W,batch_len):\n",
    "    grad = []\n",
    "    for i in range(0, batch_len):\n",
    "        ab = np.matmul(grad_a[i], W.T)\n",
    "        grad.append(ab)\n",
    "    grad = np.asarray(grad)\n",
    "    return grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, no_of_iterations, learning_rate):\n",
    "    loss_list=[]\n",
    "    i_epoch = 0\n",
    "    W1 = np.random.normal(0.0,1.0,(input_dim,hidden_1_dim))\n",
    "    W2 = np.random.normal(0.0,1.0,(hidden_1_dim,hidden_2_dim))\n",
    "    W3 = np.random.normal(0.0,1.0,(hidden_2_dim,output_dim))\n",
    "    for i_iter in range(no_of_iterations):\n",
    "    \n",
    "        ''''''\n",
    "        # getting one batch\n",
    "        batch_elem_idx = i_iter%num_minibatches\n",
    "        x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "        y_batchinput = y_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "        y_indices = gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "\n",
    "        ############################## feed forward ##################################\n",
    "        # first hidden layer implementation\n",
    "        a1 = compute_act(W1,x_batchinput,batch_size,hidden_1_dim)\n",
    "        #print(\"Shape (a1): \",a1.shape)\n",
    "        # implement Relu layer\n",
    "        h1 = relu(a1)\n",
    "        #  implement 2 hidden layer\n",
    "        a2 = compute_act(W2,h1,batch_size,hidden_2_dim)\n",
    "        #print(\"Shape (a2): \",a2.shape)\n",
    "        # implement Relu activation \n",
    "        h2 = relu(a2)\n",
    "        #implement linear output layer\n",
    "        a3 = compute_act(W3,h2,batch_size,output_dim)\n",
    "        #print(\"Shape (a3): \",a3.shape)\n",
    "        \n",
    "        \n",
    "        # softmax layer\n",
    "        softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "        #print(\"Shape (softmax): \",softmax_score.shape)\n",
    "    \n",
    "        ##################################################################################\n",
    "        ###############################################################################################\n",
    "    \n",
    "        neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "        #print(\"Shape loss = -log(softmax_score): \",neg_log_softmax_score.shape)\n",
    "        \n",
    "        # Compute and print loss\n",
    "        if i_iter%num_minibatches == 0:\n",
    "            loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                           axis=1)))\n",
    "            print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "            loss_list.append(loss)\n",
    "            i_epoch += 1\n",
    "            # Each 10th epoch reduce learning rate by a factor of 10\n",
    "            if i_epoch%10 == 0:\n",
    "                learning_rate /= 10.0\n",
    "        \n",
    "        ################################### Backpropagation #####################################\n",
    "        # Gradient of cross-entropy loss w.r.t. preactivation of the output layer\n",
    "        grad_softmax_score = grad_softmax(softmax_score, y_batchinput, y_indices, batch_size)\n",
    "        # gradient w.r.t W3\n",
    "        grad_W3 = grad_W_3(h2,grad_softmax_score,W3)\n",
    "        # gradient w.r.t h2\n",
    "        grad_h2 = grad_hidd2(softmax_score,y_batchinput,W3,batch_size)\n",
    "        # gradient w.r.t a2\n",
    "        grad_a2 = grad_h2 * mask(a2)\n",
    "        # gradient w.r.t W2\n",
    "        grad_W2 = grad_W(h1,hidden_1_dim,grad_a2,hidden_2_dim,batch_size)\n",
    "        # gradient w.r.t h1\n",
    "        grad_h1 = grad_hidd1(grad_a2,W2,batch_size)\n",
    "        # gradient w.r.t a1\n",
    "        grad_a1 = grad_h1 * mask(a1)\n",
    "        # gradient w.r.t W1\n",
    "        grad_W1 = grad_W(x_batchinput,input_dim,grad_a1,hidden_1_dim,batch_size)\n",
    "        ####################################################################################################\n",
    "        \n",
    "        ################################ Update Weights Block using SGD ####################################\n",
    "        W3 -= learning_rate * grad_W3\n",
    "        W2 -= learning_rate * grad_W2\n",
    "        W1 -= learning_rate * grad_W1\n",
    "        ####################################################################################################\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 16.9816 \n",
      " Epoch: 1, iteration: 937, Loss: 1.4391 \n",
      " Epoch: 2, iteration: 1874, Loss: 1.1435 \n",
      " Epoch: 3, iteration: 2811, Loss: 1.4391 \n",
      " Epoch: 4, iteration: 3748, Loss: 0.8635 \n",
      " Epoch: 5, iteration: 4685, Loss: 0.5758 \n",
      " Epoch: 6, iteration: 5622, Loss: 0.5756 \n"
     ]
    }
   ],
   "source": [
    "loss_list = train(x_train, y_train, 20000, learning_rate)\n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('./data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ######### copy only the forward pass block of your code and pass the x_batchinput to it and compute softmax_score ##########\n",
    "    # first hidden layer implementation\n",
    "    a1 = compute_act(W1,x_batchinput,batch_size_test,hidden_1_dim)\n",
    "    \n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(a1,0)\n",
    "    \n",
    "    #  implement 2nd hidden layer\n",
    "    a2 = compute_act(W2,h1,batch_size_test,hidden_1_dim)\n",
    "    \n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2,0)\n",
    "    \n",
    "    #implement linear output layer\n",
    "    a3 = compute_act(W3,h2,batch_size_test,hidden_1_dim)\n",
    "    \n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    \n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    y_pred = np.argmax(softmax_score, axis=1)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Hidden_MLP_New.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
